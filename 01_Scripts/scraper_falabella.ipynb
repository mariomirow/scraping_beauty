{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest-asyncio in c:\\users\\leand\\anaconda3\\lib\\site-packages (1.5.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests_html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests_html\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTMLSession \u001b[38;5;66;03m#, AsyncHTMLSession\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m date\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests_html'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from requests_html import HTMLSession #, AsyncHTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "import re\n",
    "import json\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nest_asyncio\n",
    "#nest_asyncio.apply()\n",
    "\n",
    "#r = await asession.get(links[0])\n",
    "#await r.html.arender(scrolldown=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapeando a árvore de URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(links, link_blacklist, base_url):\n",
    "\n",
    "    passlist = [l for l in links if not any(xl in l for xl in link_blacklist)] # Filtra links que contem texto constante na blacklist    \n",
    "    filtered_list = []\n",
    "    discarded_list = []\n",
    "    \n",
    "    for i in passlist:\n",
    "        if (len(i) <= 1) or i == base_url: # Descarta se for so uma barra ou o url base do boticario\n",
    "            discarded_list.append(i)\n",
    "        elif i[:len(base_url)] == base_url: # Mantem se for um url do boticario\n",
    "            filtered_list.append(i)\n",
    "        elif i[:4] == 'http':\n",
    "            discarded_list.append(i)\n",
    "        elif i[0] == '/': # Monta o url completo\n",
    "            filtered_list.append(base_url+i)\n",
    "        elif i[0] != '/': # Monta o url completo\n",
    "            filtered_list.append(base_url+'/'+i)\n",
    "        else:\n",
    "            discarded_list.append(i)\n",
    "\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_links(seed_url, base_url, link_blacklist):\n",
    "    r = session.get(seed_url)\n",
    "\n",
    "    return filter_links(r.html.links, link_blacklist, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_type(session, link):\n",
    "    try:\n",
    "        page_type = link.split('/')[4].capitalize()\n",
    "\n",
    "    except Exception as e:\n",
    "        page_type = \"Unknown\"\n",
    "        if debug_mode:print(e)\n",
    "    \n",
    "    if debug_mode:\n",
    "        print(\"Type: \", page_type)\n",
    "\n",
    "    return page_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_link(link, product_set, discard_set): # Verifica se o link ja foi visto ou nao\n",
    "    new = not(link in product_set or link in discard_set)\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_links(session, link):\n",
    "\n",
    "    r = session.get(link)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    lks = []\n",
    "    pods = soup.find_all(\"div\", {\"pod-layout\":\"4_GRID\"})\n",
    "\n",
    "    for pod in pods:\n",
    "        a = pod.find(\"a\", {\"class\":\"jsx-2907167179 layout_grid-view layout_view_4_GRID\"}).attrs['href']\n",
    "\n",
    "        if link_type(session, a) == \"Product\":\n",
    "            lks.append(a)\n",
    "\n",
    "    return lks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_mapper(session, seed_links, link_type_blacklist, link_blacklist, product_max):\n",
    "    product_set = set() # Pilha de produtos\n",
    "    discard_set = set() # Pilha de descarte\n",
    "    seed_set = set(seed_links) # Pilha de links\n",
    " \n",
    "    while (len(seed_set) > 0) and (len(product_set) < product_max):\n",
    "        link = seed_set.pop() # Pega o primeiro link da lista\n",
    "        \n",
    "        if debug_mode:\n",
    "            print(\"---------------------------------------\")\n",
    "            print(\"Product stack: \", len(product_set))\n",
    "            print(\"Discard stack: \", len(discard_set))\n",
    "            print(\"Link stack: \", len(seed_set))\n",
    "            print(\"Current link: \", link)\n",
    "        \n",
    "        p_links = get_product_links(session, link)\n",
    "        \n",
    "        old_size = len(product_set)       \n",
    "\n",
    "        for p in p_links: # Adiciona todos os produtos da pagina na pilha de produtos\n",
    "            product_set.add(p)\n",
    "\n",
    "        new_size = len(product_set)\n",
    "\n",
    "        if debug_mode:\n",
    "            print(new_size-old_size, \" new links added to product stack\")\n",
    "\n",
    "        discard_set.add(link)\n",
    "    \n",
    "    return product_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegando dados de um único produto\n",
    "\n",
    "Dados desejados:\n",
    "- País\n",
    "- Concorrente\n",
    "- Data scrape\n",
    "- ID produto\n",
    "- Título\n",
    "- Descrição\n",
    "- Preço atual\n",
    "- Preço antigo\n",
    "- Desconto atual\n",
    "- Moeda\n",
    "- Disponibilidade\n",
    "- Condição\n",
    "- Departamento\n",
    "- Categoria\n",
    "- Marca\n",
    "- Linha\n",
    "- URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_data(session, link): # A partir da pagina do item, busca todas suas informacoes pertinentes\n",
    "    r = session.get(link)\n",
    "    \n",
    "    item = BeautifulSoup(r.text, \"html.parser\")\n",
    "    \n",
    "    pais = \"Chile\"\n",
    "    competidor = \"Falabella\"\n",
    "    data = date.today().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # ID\n",
    "    try:\n",
    "        id = link.split('/')[-1]\n",
    "    except Exception as e:\n",
    "        id = None\n",
    "        if debug_mode:print(e)\n",
    "\n",
    "    # Title\n",
    "    try:\n",
    "        title = item.head.find(\"meta\", {\"property\":\"og:title\"}).attrs['content'].split('|')[0].strip()\n",
    "    except Exception as e:\n",
    "        title = None\n",
    "        if debug_mode:print(e)\n",
    "        \n",
    "    # Description\n",
    "    try:\n",
    "        description = item.head.find(\"meta\", {\"name\":\"description\"}).attrs['content']\n",
    "    except Exception as e:\n",
    "        description = None\n",
    "        if debug_mode:print(e)\n",
    "        \n",
    "    # Current price\n",
    "    try:\n",
    "        price = item.find(\"li\", {\"class\":\"jsx-749763969 prices-0\"}).attrs['data-event-price'].replace('.','')\n",
    "        price = int(price)\n",
    "    except Exception as e:\n",
    "        price = None\n",
    "        if debug_mode:print(e)\n",
    "        \n",
    "    # Previous price\n",
    "    try:\n",
    "        maxprice = item.find(\"li\", {\"class\":\"jsx-749763969 prices-1\"}).attrs['data-normal-price'].replace('.','')\n",
    "        maxprice = int(maxprice)\n",
    "    except Exception as e:\n",
    "        maxprice = None\n",
    "        if debug_mode:print(e)\n",
    "        \n",
    "    # Currency\n",
    "    try:\n",
    "        currency = 'CLP'\n",
    "    except Exception as e:\n",
    "        currency = None\n",
    "        if debug_mode:print(e)\n",
    "        \n",
    "    # Seller\n",
    "    try:\n",
    "        seller = item.find(\"a\", {\"id\":\"testId-SellerInfo-sellerName\"}).attrs['href'].split('/')[-1]\n",
    "        seller = unquote(unquote(seller))\n",
    "    except Exception as e:\n",
    "        seller = None\n",
    "        if debug_mode:print(e)\n",
    "        \n",
    "    # URL\n",
    "    url = link\n",
    "\n",
    "    # Other attributes\n",
    "    try:\n",
    "        attrs = item.find_all(\"tr\", {\"class\":\"jsx-428502957\"})\n",
    "        tempDict = {}\n",
    "        for att in attrs:\n",
    "            k = att.contents[0].text\n",
    "            v = att.contents[1].text\n",
    "            tempDict[k] = v\n",
    "\n",
    "    except Exception as e:\n",
    "        tempDict = {}\n",
    "        if debug_mode:print(e)    \n",
    "\n",
    "    d = {\t\n",
    "            \"País\":pais,\n",
    "            \"Concorrente\":competidor,\n",
    "            \"Data scrape\":data,\n",
    "            \"ID produto\":id,\n",
    "            \"Título\":title,\n",
    "            \"Descrição\":description,\n",
    "            \"Preço atual\":price,\n",
    "            \"Preço antigo\":maxprice,\n",
    "            \"Moeda\":currency,\n",
    "            \"Vendedor\":seller,\n",
    "            \"URL\":url\n",
    "        }\n",
    "\n",
    "    d.update(tempDict) # Inclui demais atributos\n",
    "\n",
    "    return pd.Series(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_items(session, links):\n",
    "    df = pd.DataFrame()\n",
    "    for link in links:\n",
    "        try:\n",
    "            item_data = get_item_data(session, link)\n",
    "            df = df.append(item_data, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            if debug_mode: print(e)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(max_pages = 200, product_max = 10_000_000):\n",
    "\n",
    "    session = HTMLSession()\n",
    "    #asession = AsyncHTMLSession()\n",
    "    base_url = \"https://www.falabella.com\"\n",
    "    seed_url = \"https://www.falabella.com/falabella-cl/category/cat7660002/Belleza--higiene-y-salud\"\n",
    "\n",
    "    link_blacklist = []\n",
    "    link_type_blacklist = ['Unknown']\n",
    "\n",
    "    \n",
    "    # Scraping\n",
    "    seed_links = []\n",
    "    for i in range(1, max_pages+1):\n",
    "        j = seed_url + '?page=' + str(i)\n",
    "        seed_links.append(j)\n",
    "    \n",
    "    products = product_mapper(session, seed_links, link_type_blacklist, link_blacklist, product_max)\n",
    "\n",
    "    df = get_all_items(session, products)\n",
    "    \n",
    "    # Reordenando\n",
    "    first = ['País', 'Concorrente', 'Data scrape', 'ID produto', 'Título', 'Descrição', 'Preço atual', 'Preço antigo', 'Moeda', 'Vendedor', 'URL'] \n",
    "    cols = first + sorted([c for c in df.columns.to_list() if c not in first])   \n",
    "\n",
    "    try:\n",
    "        df = df[cols]\n",
    "    except Exception as e:\n",
    "        if debug_mode: print(e)\n",
    "    \n",
    "    df.to_excel(\"../02_Results/\"+date.today().strftime(\"%Y_%m_%d\")+\"_\"+\"falabella.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
